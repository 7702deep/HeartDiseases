{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Heart Disease Prediction.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KlFebVy0a1p8","colab_type":"text"},"source":["# Heart Disease Prediction\n","\n","In this machine learning project, I have collected the dataset from Kaggle (https://www.kaggle.com/ronitf/heart-disease-uci) and I will be using Machine Learning to make predictions on whether a person is suffering from Heart Disease or not."]},{"cell_type":"markdown","metadata":{"id":"z07KYFRPa1qK","colab_type":"text"},"source":["### Import libraries\n","\n","Let's first import all the necessary libraries. I'll use `numpy` and `pandas` to start with. For visualization, I will use `pyplot` subpackage of `matplotlib`, use `rcParams` to add styling to the plots and `rainbow` for colors. For implementing Machine Learning models and processing of data, I will use the `sklearn` library."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xyCNfxdUa9h9","colab":{},"executionInfo":{"status":"ok","timestamp":1600762411641,"user_tz":-330,"elapsed":938,"user":{"displayName":"sasi phani reddy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2ScpY-QQTGLWbMqnm1XDUZQA1I3C_fv_vTgVSew=s64","userId":"04844267718565978640"}}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import rcParams\n","from matplotlib.cm import rainbow\n","%matplotlib inline\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XAKwndRma1qi","colab_type":"text"},"source":["For processing the data, I'll import a few libraries. To split the available dataset for testing and training, I'll use the `train_test_split` method. To scale the features, I am using `StandardScaler`."]},{"cell_type":"code","metadata":{"id":"jFjJeZ1Wa1qo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600762418248,"user_tz":-330,"elapsed":1513,"user":{"displayName":"sasi phani reddy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2ScpY-QQTGLWbMqnm1XDUZQA1I3C_fv_vTgVSew=s64","userId":"04844267718565978640"}}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xa46gscOa1q-","colab_type":"text"},"source":["Next, I'll import all the Machine Learning algorithms I will be using.\n","1. K Neighbors Classifier\n","2. Support Vector Classifier\n","3. Decision Tree Classifier\n","4. Random Forest Classifier"]},{"cell_type":"code","metadata":{"id":"KhZulUWQa1rG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600762421105,"user_tz":-330,"elapsed":1561,"user":{"displayName":"sasi phani reddy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2ScpY-QQTGLWbMqnm1XDUZQA1I3C_fv_vTgVSew=s64","userId":"04844267718565978640"}}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26V-bHnka1re","colab_type":"text"},"source":["### Import dataset\n","\n","Now that we have all the libraries we will need, I can import the dataset and take a look at it. The dataset is stored in the file `dataset.csv`. I'll use the pandas `read_csv` method to read the dataset."]},{"cell_type":"code","metadata":{"id":"0i0JRKWBa1rl","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv('heart.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3GcqGmSa1r6","colab_type":"text"},"source":["The dataset is now loaded into the variable `dataset`. I'll just take a glimpse of the data using the `desribe()` and `info()` methods before I actually start processing and visualizing it."]},{"cell_type":"code","metadata":{"id":"Fh1wxy1ya1sB","colab_type":"code","colab":{}},"source":["dataset.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqmNnhkSa1sY","colab_type":"text"},"source":["Looks like the dataset has a total of 303 rows and there are no missing values. There are a total of `13 features` along with one target value which we wish to find."]},{"cell_type":"code","metadata":{"id":"IWkRUiKaa1se","colab_type":"code","colab":{}},"source":["dataset.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWYeqePQa1sv","colab_type":"text"},"source":["The scale of each feature column is different and quite varied as well. While the maximum for `age` reaches 77, the maximum of `chol` (serum cholestoral) is 564."]},{"cell_type":"markdown","metadata":{"id":"nmXYvHpma1sz","colab_type":"text"},"source":["### Understanding the data\n","\n","Now, we can use visualizations to better understand our data and then look at any processing we might want to do."]},{"cell_type":"code","metadata":{"id":"oxI3Szzia1s4","colab_type":"code","colab":{}},"source":["rcParams['figure.figsize'] = 20, 14\n","plt.matshow(dataset.corr())\n","plt.yticks(np.arange(dataset.shape[1]), dataset.columns)\n","plt.xticks(np.arange(dataset.shape[1]), dataset.columns)\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ZkkoBkCa1tI","colab_type":"text"},"source":["Taking a look at the correlation matrix above, it's easy to see that a few features have negative correlation with the target value while some have positive.\n","Next, I'll take a look at the histograms for each variable."]},{"cell_type":"code","metadata":{"id":"fBXoYcrAa1tM","colab_type":"code","colab":{}},"source":["dataset.hist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7JZSjZtca1ta","colab_type":"text"},"source":["Taking a look at the histograms above, I can see that each feature has a different range of distribution. Thus, using scaling before our predictions should be of great use. Also, the categorical features do stand out."]},{"cell_type":"markdown","metadata":{"id":"YviNSLxPa1te","colab_type":"text"},"source":["It's always a good practice to work with a dataset where the target classes are of approximately equal size. Thus, let's check for the same."]},{"cell_type":"code","metadata":{"id":"D5Y4E27Wa1tj","colab_type":"code","colab":{}},"source":["rcParams['figure.figsize'] = 8,6\n","plt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\n","plt.xticks([0, 1])\n","plt.xlabel('Target Classes')\n","plt.ylabel('Count')\n","plt.title('Count of each Target Class')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzafBXkKa1tv","colab_type":"text"},"source":["The two classes are not exactly 50% each but the ratio is good enough to continue without dropping/increasing our data."]},{"cell_type":"markdown","metadata":{"id":"o-seqgvia1tz","colab_type":"text"},"source":["### Data Processing\n","\n","After exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models.\n","First, I'll use the `get_dummies` method to create dummy columns for categorical variables."]},{"cell_type":"code","metadata":{"id":"IV0qgnRXa1t3","colab_type":"code","colab":{}},"source":["dataset = pd.get_dummies(dataset, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZpIo6gUa1uD","colab_type":"text"},"source":["Now, I will use the `StandardScaler` from `sklearn` to scale my dataset."]},{"cell_type":"code","metadata":{"id":"F9D2OgNXa1uI","colab_type":"code","colab":{}},"source":["standardScaler = StandardScaler()\n","columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n","dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-KNomZNOa1uW","colab_type":"text"},"source":["The data is not ready for our Machine Learning application."]},{"cell_type":"markdown","metadata":{"id":"DoYWpifpa1ua","colab_type":"text"},"source":["### Machine Learning\n","\n","I'll now import `train_test_split` to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data."]},{"cell_type":"code","metadata":{"id":"ZLMMNQ_oa1ud","colab_type":"code","colab":{}},"source":["y = dataset['target']\n","X = dataset.drop(['target'], axis = 1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVYjWjJka1uq","colab_type":"text"},"source":["#### K Neighbors Classifier\n","\n","The classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score."]},{"cell_type":"code","metadata":{"id":"9P4noGFSa1uu","colab_type":"code","colab":{}},"source":["knn_scores = []\n","for k in range(1,21):\n","    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n","    knn_classifier.fit(X_train, y_train)\n","    knn_scores.append(knn_classifier.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7e9nlYr-a1u5","colab_type":"text"},"source":["I have the scores for different neighbor values in the array `knn_scores`. I'll now plot it and see for which value of K did I get the best scores."]},{"cell_type":"code","metadata":{"id":"89VGM9wla1u_","colab_type":"code","colab":{}},"source":["plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\n","for i in range(1,21):\n","    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\n","plt.xticks([i for i in range(1, 21)])\n","plt.xlabel('Number of Neighbors (K)')\n","plt.ylabel('Scores')\n","plt.title('K Neighbors Classifier scores for different K values')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-N8FqaXa1vO","colab_type":"text"},"source":["From the plot above, it is clear that the maximum score achieved was `0.87` for the 8 neighbors."]},{"cell_type":"code","metadata":{"id":"jWTxtRHMa1vS","colab_type":"code","colab":{}},"source":["print(\"The score for K Neighbors Classifier is {}% with {} nieghbors.\".format(knn_scores[7]*100, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ppPxoTra1vd","colab_type":"text"},"source":["#### Support Vector Classifier\n","\n","There are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score."]},{"cell_type":"code","metadata":{"id":"4D0M17P9a1vh","colab_type":"code","colab":{}},"source":["svc_scores = []\n","kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n","for i in range(len(kernels)):\n","    svc_classifier = SVC(kernel = kernels[i])\n","    svc_classifier.fit(X_train, y_train)\n","    svc_scores.append(svc_classifier.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aT5EaEcya1vu","colab_type":"text"},"source":["I'll now plot a bar plot of scores for each kernel and see which performed the best."]},{"cell_type":"code","metadata":{"id":"V_DD2Rqra1vx","colab_type":"code","colab":{}},"source":["colors = rainbow(np.linspace(0, 1, len(kernels)))\n","plt.bar(kernels, svc_scores, color = colors)\n","for i in range(len(kernels)):\n","    plt.text(i, svc_scores[i], svc_scores[i])\n","plt.xlabel('Kernels')\n","plt.ylabel('Scores')\n","plt.title('Support Vector Classifier scores for different kernels')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"InADmPY3a1v8","colab_type":"text"},"source":["The `linear` kernel performed the best, being slightly better than `rbf` kernel."]},{"cell_type":"code","metadata":{"id":"iVRdG-S8a1v_","colab_type":"code","colab":{}},"source":["print(\"The score for Support Vector Classifier is {}% with {} kernel.\".format(svc_scores[0]*100, 'linear'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ellY53Pa1wL","colab_type":"text"},"source":["#### Decision Tree Classifier\n","\n","Here, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of `max_features` and see which returns the best accuracy."]},{"cell_type":"code","metadata":{"id":"x7nnVQlma1wO","colab_type":"code","colab":{}},"source":["dt_scores = []\n","for i in range(1, len(X.columns) + 1):\n","    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n","    dt_classifier.fit(X_train, y_train)\n","    dt_scores.append(dt_classifier.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJqUSs3oa1wY","colab_type":"text"},"source":["I selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases."]},{"cell_type":"code","metadata":{"id":"Y7lrBUAUa1wc","colab_type":"code","colab":{}},"source":["plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\n","for i in range(1, len(X.columns) + 1):\n","    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\n","plt.xticks([i for i in range(1, len(X.columns) + 1)])\n","plt.xlabel('Max features')\n","plt.ylabel('Scores')\n","plt.title('Decision Tree Classifier scores for different number of maximum features')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31JD4ZkEa1wn","colab_type":"text"},"source":["The model achieved the best accuracy at three values of maximum features, `2`, `4` and `18`."]},{"cell_type":"code","metadata":{"id":"g_xXnQ3_a1wq","colab_type":"code","colab":{}},"source":["print(\"The score for Decision Tree Classifier is {}% with {} maximum features.\".format(dt_scores[17]*100, [2,4,18]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BI8L6Wm9a1w1","colab_type":"text"},"source":["#### Random Forest Classifier\n","\n","Now, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect."]},{"cell_type":"code","metadata":{"id":"CPZex3bCa1w4","colab_type":"code","colab":{}},"source":["rf_scores = []\n","estimators = [10, 100, 200, 500, 1000]\n","for i in estimators:\n","    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n","    rf_classifier.fit(X_train, y_train)\n","    rf_scores.append(rf_classifier.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suf03-0oa1xE","colab_type":"text"},"source":["The model is trained and the scores are recorded. Let's plot a bar plot to compare the scores."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"esNjeK-ta1xH","colab_type":"code","colab":{}},"source":["colors = rainbow(np.linspace(0, 1, len(estimators)))\n","plt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\n","for i in range(len(estimators)):\n","    plt.text(i, rf_scores[i], rf_scores[i])\n","plt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\n","plt.xlabel('Number of estimators')\n","plt.ylabel('Scores')\n","plt.title('Random Forest Classifier scores for different number of estimators')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1F8m1n20a1xX","colab_type":"text"},"source":["The maximum score is achieved when the total estimators are 100 or 500."]},{"cell_type":"code","metadata":{"id":"8Tb6l1tha1xb","colab_type":"code","colab":{},"outputId":"6001059a-fb58-4f6a-c8f4-121babbc5f62"},"source":["print(\"The score for Random Forest Classifier is {}% with {} estimators.\".format(rf_scores[1]*100, [100, 500]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The score for Random Forest Classifier is 84.0% with [100, 500] estimators.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XWmCbKMJa1xm","colab_type":"text"},"source":["### Conclusion\n","\n","In this project, I used Machine Learning to predict whether a person is suffering from a heart disease. After importing the data, I analysed it using plots. Then, I did generated dummy variables for categorical features and scaled other features. \n","I then applied four Machine Learning algorithms, `K Neighbors Classifier`, `Support Vector Classifier`, `Decision Tree Classifier` and `Random Forest Classifier`. I varied parameters across each model to improve their scores.\n","In the end, `K Neighbors Classifier` achieved the highest score of `87%` with `8 nearest neighbors`."]}]}